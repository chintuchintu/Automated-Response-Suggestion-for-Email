{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Smart_email_reply_suggestion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yatindma/Automated-Response-Suggestion-for-Email/blob/master/Smart_email_reply_suggestion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c-6YuFhv-JH",
        "colab_type": "text"
      },
      "source": [
        "Smart Email Reply Suggestion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eob1MGEiJKb1",
        "colab_type": "text"
      },
      "source": [
        "In this case study,<br> we have to suggest 3 replies to the user,<br> so that user need not to type the reply"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zq791T8qM4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.python.keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04fl9zoQWSMn",
        "colab_type": "code",
        "outputId": "6c4605bc-5b50-438c-a372-459b2f6d0852",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ3o8wZwq-7-",
        "colab_type": "text"
      },
      "source": [
        "Reading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNW5nOAEq0vj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#Reading email dataset\n",
        "data = pd.read_csv('qa_dataset.csv', encoding = \"ISO-8859-1\", low_memory=False)\n",
        "#Dropping unused data from the dataframe \n",
        "data = data.drop(['ArticleTitle','DifficultyFromQuestioner','DifficultyFromAnswerer','ArticleFile'],axis =1 ) \n",
        "#Changing teh name of teh collumns\n",
        "data.columns = ['question','reply']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-rujaNTrCFq",
        "colab_type": "code",
        "outputId": "f974b354-0024-4140-dfe5-eb32787a8392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>reply</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Was Volta an Italian physicist?</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Is Volta buried in the city of Pittsburgh?</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Did Volta have a passion for the study of elec...</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the battery made by Volta credited to be?</td>\n",
              "      <td>the first cell</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What important electrical unit was named in ho...</td>\n",
              "      <td>the volt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question           reply\n",
              "0                    Was Volta an Italian physicist?             yes\n",
              "1         Is Volta buried in the city of Pittsburgh?              no\n",
              "2  Did Volta have a passion for the study of elec...             yes\n",
              "3  What is the battery made by Volta credited to be?  the first cell\n",
              "4  What important electrical unit was named in ho...        the volt"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on9Jb9_lrjn6",
        "colab_type": "text"
      },
      "source": [
        "Converting data into lower case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a33I75XrsmD",
        "colab_type": "code",
        "outputId": "c4321d9c-85c7-4c8d-ff62-1d4c169282fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "data = data.apply(lambda x: x.astype(str).str.lower())\n",
        "data.head(4)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>reply</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>was volta an italian physicist?</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is volta buried in the city of pittsburgh?</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>did volta have a passion for the study of elec...</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what is the battery made by volta credited to be?</td>\n",
              "      <td>the first cell</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question           reply\n",
              "0                    was volta an italian physicist?             yes\n",
              "1         is volta buried in the city of pittsburgh?              no\n",
              "2  did volta have a passion for the study of elec...             yes\n",
              "3  what is the battery made by volta credited to be?  the first cell"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZJl1i9crzmH",
        "colab_type": "text"
      },
      "source": [
        "Removing all special characters from the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd9bIzfSrVTq",
        "colab_type": "code",
        "outputId": "b6765c67-8a1a-48ad-9e83-28b20c78019f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import re\n",
        "all_questions = []\n",
        "for sentence in data.question:\n",
        "   all_questions.append(re.sub('[^A-Za-z0-9]+', ' ', sentence))\n",
        "print(all_questions[:3]) # to check wheather the removal code didn' effected the data"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['was volta an italian physicist ', 'is volta buried in the city of pittsburgh ', 'did volta have a passion for the study of electricity ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE4Q2u1LrvXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Putting all preprocessed data back to the dataframe\n",
        "data['question'] = all_questions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srTXDp1csAuE",
        "colab_type": "text"
      },
      "source": [
        "Trimming the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYtcjFIXr9wC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data.applymap(lambda x: x.strip() if isinstance(x, str) else x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMOBU1ejsHcl",
        "colab_type": "text"
      },
      "source": [
        "Dropping Duplicate questions from the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE_P599HsD14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.drop_duplicates(subset =\"question\", keep = False, inplace = True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENqOKoyWuX0Q",
        "colab_type": "text"
      },
      "source": [
        "Remove the data from which are having more than 20 words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAzFcMZZvmVD",
        "colab_type": "code",
        "outputId": "da0f5fce-832b-495a-ab3a-9d832c9676f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "temp_arr = data[\"reply\"].str.len()\n",
        "# get the average of the length of the replies\n",
        "total = sum(temp_arr)\n",
        "average_length = total / len(temp_arr)\n",
        "print(\"average length of the replies\",int(average_length))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average length of the replies 17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5-JPhD-wD_G",
        "colab_type": "text"
      },
      "source": [
        "so after seeing the average length of the we'll choose max length i.e 20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjAb-HmLulbc",
        "colab_type": "code",
        "outputId": "c141b170-c37c-42c4-997f-3454c4ce5c91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "#get the length of the replies\n",
        "data[\"reply_length\"]= data[\"reply\"].str.len()\n",
        "\n",
        "data = data[(data.reply_length < 20)]\n",
        "print(data.head())"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                            question  ... reply_length\n",
            "0                     was volta an italian physicist  ...            3\n",
            "1          is volta buried in the city of pittsburgh  ...            2\n",
            "2  did volta have a passion for the study of elec...  ...            3\n",
            "3   what is the battery made by volta credited to be  ...           14\n",
            "6                   where did volta enter retirement  ...            5\n",
            "\n",
            "[5 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_UG5C5GDfNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r58shfO_sQbi",
        "colab_type": "text"
      },
      "source": [
        "**DATA VISUALIZATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aVDO3G8X2DjP",
        "colab": {}
      },
      "source": [
        "reply_list = list(data['reply'].values)\n",
        "cleaned_replies = []\n",
        "for sentence in reply_list:\n",
        "   cleaned_replies.append(re.sub('[^A-Za-z0-9]+', ' ', sentence))\n",
        "\n",
        "reply_dict = {i:reply_list.count(i) for i in cleaned_replies}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yNfvv1i12DjY",
        "colab": {}
      },
      "source": [
        "from collections import OrderedDict\n",
        "reply_dict_sorted = OrderedDict(sorted(reply_dict.items(), key=lambda x: x[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d32EpOzEmE7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reply = []\n",
        "keys = []\n",
        "for item in reply_dict_sorted.items():\n",
        "  reply.append(item[0])\n",
        "  keys.append(item[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oVVC9oqja57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reply.reverse()\n",
        "keys.reverse()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_Ad6vx_8LhB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#top 10 reply\n",
        "top_5_reply = reply[:5]\n",
        "top_5_keys = keys[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmMETsW6-q8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp_reply_list = []\n",
        "i = 0\n",
        "for key in top_5_keys:\n",
        "  for _ in range(0,key):\n",
        "    temp_reply_list.append(top_5_reply[i])\n",
        "  i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxzR9Cj8FhBF",
        "colab_type": "text"
      },
      "source": [
        "<b>\n",
        "Article title can be used while giving the sentence for tokenizing later \n",
        "question do we need to preprocess the data\n",
        "reply mai kitne repeated h\n",
        "</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGGxT8LNFgjE",
        "colab_type": "code",
        "outputId": "cb6463bc-1a4f-48de-f78e-669a7d9d3c3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.hist(temp_reply_list, 10,\n",
        "         histtype='bar',\n",
        "         facecolor='b',\n",
        "         alpha=0.5)\n",
        "plt.show()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3t4K_gaAT8t",
        "colab_type": "text"
      },
      "source": [
        "From the above graph we can observe that \n",
        "<b>yes</b> and <b>no</b> reply are the most common reply in the mails"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyqpWZfm6lat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#all the words \n",
        "# Vocabulary of questions\n",
        "all_email_words=[]\n",
        "for quest in data['question'].values:\n",
        "    for word in quest.split():\n",
        "        if word not in all_email_words:\n",
        "            all_email_words.append(word)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taIEFLxzybwB",
        "colab_type": "text"
      },
      "source": [
        "Tokenize the sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHVZ7tuEyeS-",
        "colab_type": "code",
        "outputId": "449f81fb-6035-4821-a616-4265be6ad929",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#All input words\n",
        "\n",
        "input_words = sorted(list(all_email_words))\n",
        "\n",
        "# get the length of the vocabalary // Kitne words // this will help while performing embedding layer\n",
        "num_encoder_tokens = len(all_email_words) + 1\n",
        "vocab_len = num_encoder_tokens\n",
        "print(vocab_len)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2988\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jx-OD99_edv",
        "colab_type": "text"
      },
      "source": [
        "**Dictionary to get token from the word**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vopiHNi4_VZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from word to token we can get\n",
        "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe8di1DD_jpw",
        "colab_type": "text"
      },
      "source": [
        "**Dictionary to get word from the token**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YwZq4TY_VfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Will not be used here\n",
        "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1_PSiyAF1Mn",
        "colab_type": "text"
      },
      "source": [
        "**We'll tokenize the sentence itself for replies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3td7ULD5F7aI",
        "colab_type": "code",
        "outputId": "88c00583-d8e7-41c6-e980-3826f2153ea1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenized_reply_sentences = []\n",
        "# need to get all the unique replies in the dataset\n",
        "\n",
        "all_unique_replies = list(set(data['reply'].values))\n",
        "print('unique replies in the data',len(all_unique_replies))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unique replies in the data 355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y48LH0OiF7jU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_words = sorted(list(all_unique_replies))\n",
        "# from word to token we can get\n",
        "output_token_index = dict([(word, i+1) for i, word in enumerate(output_words)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_dUnk5TT7ia",
        "colab_type": "code",
        "outputId": "07ee922b-5c0f-47ca-d9c8-d04cc9cfbd09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "output_token_index['the duck']"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "314"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnCdioyfF7qW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "reverse_output_index = dict((i, word) for word, i in output_token_index.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmj6s_qrs1Pf",
        "colab_type": "text"
      },
      "source": [
        "**Divide the data into train and test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SYT--bps4qY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(data['question'], data['reply'], test_size = 0.1, random_state = 42)\n",
        "#we have given only 10% of the data for the testing as already we have very less data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12I5WwEasdzw",
        "colab_type": "code",
        "outputId": "3b763d78-e79e-4179-fa1c-ff1a08a19065",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Checking number of unique replies that dataset contain\n",
        "all_unique_replies = list(set(data['reply'].values))\n",
        "print('unique replies in the data',len(all_unique_replies))\n",
        "\n",
        "all_unique_replies_train = list(set(y_train.values))\n",
        "print('unique replies in the train data',len(all_unique_replies_train))\n",
        "\n",
        "all_unique_replies_test = list(set(y_test.values))\n",
        "print('unique replies in the test data',len(all_unique_replies_test))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unique replies in the data 355\n",
            "unique replies in the train data 327\n",
            "unique replies in the test data 41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLYPuTML_LRV",
        "colab_type": "text"
      },
      "source": [
        "Need to check how many sentences which are there in test replies but are not present in the train data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8mOEgHYCeyY",
        "colab_type": "code",
        "outputId": "0ab366ac-fb1c-4deb-f2a4-424dbdaffdce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "replies_in_test_not_available_in_train_set = len(all_unique_replies) - len(all_unique_replies_train)\n",
        "print(\"Replies not available in train data\",replies_in_test_not_available_in_train_set)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Replies not available in train data 28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voW8gKx7AdLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch(X,y, batch_size):\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "                    encoder_input_data = np.zeros((batch_size, vocab_len),\n",
        "                            dtype='float32')\n",
        "                    rows, cols = (len(y), len(all_unique_replies)) \n",
        "                    # output_data = [[0]*cols]*rows \n",
        "                    output_data = np.zeros((rows, cols),\n",
        "                            dtype='float32')\n",
        "                    for (i, (input_text, target_text)) in enumerate(zip(X[j:j+ batch_size], y[j:j + batch_size])):\n",
        "\n",
        "                        # for words in each sentence //question\n",
        "                        for (t, word) in enumerate(input_text.split()):  # question wale text ko tokenize kiya gaya\n",
        "                            encoder_input_data[i,t] = input_token_index[word]  # encoder input seq\n",
        "                        #For replies we are creating this\n",
        "                        \n",
        "                        ##output_data[i,output_token_index[target_text]] = 1\n",
        "                        output_data[i][output_token_index[target_text]-1] = 1\n",
        "                    return (encoder_input_data,output_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6wcVpuIBy0c",
        "colab_type": "text"
      },
      "source": [
        "Glove Vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQIAIpZiAdFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "glove_pickel = open(\"drive/My Drive/DonarChoose/glove_vectors\",\"rb\")\n",
        "glove_ = pickle.load(glove_pickel)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyCDyh_aAdCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vector = glove_.get(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETAD11YZAdAO",
        "colab_type": "code",
        "outputId": "dad80220-45c7-489b-c002-b985d5487e2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embedded_matrix = np.zeros((vocab_len, 300))\n",
        "for word, i in input_token_index.items():\n",
        "    vector = glove_.get(word)\n",
        "    if vector is not None:\n",
        "      embedded_matrix[i] = vector\n",
        "print(embedded_matrix.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2988, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0fG9mYGCFtu",
        "colab_type": "text"
      },
      "source": [
        "#Create model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpbrYt8-Ac5r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "b885bc57-f5c6-4775-ccfc-d38623ee7c60"
      },
      "source": [
        "#https://stackoverflow.com/questions/56097089/how-to-fix-name-embedding-is-not-defined-in-keras\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Input, Flatten, concatenate,Embedding,RepeatVector\n",
        "latent_dim = 300\n",
        "embedded_layer = Embedding(vocab_len,latent_dim,weights=[embedded_matrix],input_length=vocab_len,trainable=False) # we are using the embedded matrix to get the weights which we initialize randomly in embedded layer"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ23fAekVWyL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "44f1b90f-13df-458d-f059-470e6479f943"
      },
      "source": [
        "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional,Concatenate,TimeDistributed\n",
        "from keras.layers import Input, LSTM, Embedding, Dense\n",
        "from keras.models import Model, Sequential\n",
        "\n",
        " # how much dimension of output we want form the embedding layer\n",
        "encoder_inputs = Input(shape=(None,)) # mentioning the input shape row and collumns are still null\n",
        "model = Sequential()\n",
        "model.add(embedded_layer)\n",
        "model.add(Bidirectional(LSTM(128))) # return a single vector of 100 dimension\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(355, activation='sigmoid')) #355 because we have 355 unique replies in the data\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHlGDRGyZDk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x,y = generate_batch(x_train,y_train,len(x_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM-LNm__XKQS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e6d63182-e8b4-45c3-ca5c-dc5eeda8240b"
      },
      "source": [
        "# model.fit(x,y)\n",
        "model.fit(x, y,\n",
        "          batch_size=100,\n",
        "          epochs=5)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1029/1029 [==============================] - 94s 91ms/step - loss: 0.4548 - acc: 0.9373\n",
            "Epoch 2/5\n",
            "1029/1029 [==============================] - 94s 91ms/step - loss: 0.1310 - acc: 0.9968\n",
            "Epoch 3/5\n",
            "1029/1029 [==============================] - 93s 91ms/step - loss: 0.0359 - acc: 0.9971\n",
            "Epoch 4/5\n",
            "1029/1029 [==============================] - 93s 90ms/step - loss: 0.0176 - acc: 0.9971\n",
            "Epoch 5/5\n",
            "1029/1029 [==============================] - 94s 91ms/step - loss: 0.0136 - acc: 0.9971\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f82e81e03c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfxFj3R4pJdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch_test(X,y):\n",
        "    while True:\n",
        "        for j in range(0, len(X), 1):\n",
        "                    encoder_input_data = np.zeros((1, vocab_len),\n",
        "                            dtype='float32')\n",
        "                    rows, cols = (len(y), len(all_unique_replies)) \n",
        "                    # output_data = [[0]*cols]*rows \n",
        "                    output_data = np.zeros((rows, cols),\n",
        "                            dtype='float32')\n",
        "                    for (i, (input_text, target_text)) in enumerate(zip(X[j:j+ 1], y[j:j + 1])):\n",
        "\n",
        "                        # for words in each sentence //question\n",
        "                        for (t, word) in enumerate(input_text.split()):  # question wale text ko tokenize kiya gaya\n",
        "                            encoder_input_data[i,t] = input_token_index[word]  # encoder input seq\n",
        "\n",
        "                        return encoder_input_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3STb8B8oTgb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GvPr1fzaGhG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "fc3d4c3c-dc41-4779-9f1d-d960d7e2a669"
      },
      "source": [
        "index = 0\n",
        "test_question = generate_batch_test(x_test.values[index],y_test.values[index])\n",
        "output_token = model.predict(test_question)\n",
        "\n",
        "#first reply #Taking first reply from the array what model has predicted \n",
        "sampled_token_index_1 = np.argmax(output_token)\n",
        "#Making highest probability as 0 so that next time we can get the 2nd highest probabily\n",
        "output_token[0][sampled_token_index_1] = 0\n",
        "\n",
        "#second reply\n",
        "sampled_token_index_2 = np.argmax(output_token)\n",
        "output_token[0][sampled_token_index_2] = 0 # making second the probability 0 so that we can get third heighest probabilty\n",
        "\n",
        "#Third reply\n",
        "sampled_token_index_3 = np.argmax(output_token)\n",
        "output_token[0][sampled_token_index_3] = 0\n",
        "\n",
        "print('question is : ',x_test.values[index])\n",
        "for i in range(0,3):\n",
        "  print('reply',i+1 ,reverse_output_index[i+1])\n",
        "\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "question is :  is rainfall very low\n",
            "reply 1 (bad question)\n",
            "reply 2 (what?)\n",
            "reply 3 *shrug*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BoD91zxFfFh",
        "colab_type": "text"
      },
      "source": [
        "Approach:\n",
        "\n",
        "1.   First we took the data and cleaned it\n",
        "2.   We tokenized each question(words/vocab) and we got one matrix for questions\n",
        "3.   Again I tokenized replies(full unique sentence has one token) <br> **eg. [how are you : 23],<br>    [Is it raining : 53]**\n",
        "4.   We gave the x_train question matrix as a tokenized and padded and for replies we have matrix for each sentence and passed it to the model\n",
        "5.   Model gave 99.7% accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASKapFeaHqZI",
        "colab_type": "text"
      },
      "source": [
        "Reference:<br>\n",
        "[Gabriel farah QA bot](https://github.com/gabrielfarah/QA_Bot)\n",
        "<br>\n",
        "[Google smart reply suggestions](https://arxiv.org/pdf/1606.04870.pdf)<br>\n",
        "[StackOverflow](https://www.stackoverflow.com)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pDRd0_cIboW",
        "colab_type": "text"
      },
      "source": [
        "> ### ### ###  Thank You ### ### ###\n"
      ]
    }
  ]
}